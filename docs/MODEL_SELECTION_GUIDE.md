# Churn ì˜ˆì¸¡ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

## ğŸ“Š Churn ì˜ˆì¸¡ì— ì í•©í•œ ëª¨ë¸ë“¤

### 1. ê¸°ë³¸ ëª¨ë¸ (Base Models)

#### 1.1 Logistic Regression
- **ì¥ì **: í•´ì„ ê°€ëŠ¥ì„± ë†’ìŒ, ë¹ ë¥¸ í•™ìŠµ ì†ë„, ì•ˆì •ì 
- **ë‹¨ì **: ë¹„ì„ í˜• ê´€ê³„ í¬ì°© ì–´ë ¤ì›€
- **ì í•©ì„±**: â­â­â­ (ê¸°ë³¸ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ì¢‹ìŒ)

#### 1.2 Random Forest
- **ì¥ì **: ë¹„ì„ í˜• ê´€ê³„ í¬ì°©, ë³€ìˆ˜ ì¤‘ìš”ë„ ì œê³µ, ê³¼ì í•© ë°©ì§€
- **ë‹¨ì **: í•´ì„ ì–´ë ¤ì›€, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í¼
- **ì í•©ì„±**: â­â­â­â­ (Churn ì˜ˆì¸¡ì— ë§¤ìš° ì í•©)

#### 1.3 Gradient Boosting
- **ì¥ì **: ìˆœì°¨ì  ì˜¤ì°¨ ë³´ì •ìœ¼ë¡œ ë†’ì€ ì„±ëŠ¥
- **ë‹¨ì **: í•™ìŠµ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘ìš”
- **ì í•©ì„±**: â­â­â­â­ (ì„±ëŠ¥ ìš°ìˆ˜)

#### 1.4 XGBoost
- **ì¥ì **: ì •ê·œí™” í¬í•¨, ë¹ ë¥¸ í•™ìŠµ, ë†’ì€ ì„±ëŠ¥
- **ë‹¨ì **: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”
- **ì í•©ì„±**: â­â­â­â­â­ (Churn ì˜ˆì¸¡ ìµœì )

#### 1.5 LightGBM
- **ì¥ì **: ë§¤ìš° ë¹ ë¥¸ í•™ìŠµ ì†ë„, ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ ìš°ìˆ˜, ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- **ë‹¨ì **: ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ ê³¼ì í•© ê°€ëŠ¥
- **ì í•©ì„±**: â­â­â­â­â­ (ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ìµœì )

#### 1.6 CatBoost
- **ì¥ì **: ë²”ì£¼í˜• ë³€ìˆ˜ ìë™ ì²˜ë¦¬, ê³¼ì í•© ë°©ì§€ ìš°ìˆ˜, íŠœë‹ ìµœì†Œí™”
- **ë‹¨ì **: í•™ìŠµ ì‹œê°„ ìƒëŒ€ì ìœ¼ë¡œ ì˜¤ë˜ ê±¸ë¦¼
- **ì í•©ì„±**: â­â­â­â­â­ (ë²”ì£¼í˜• ë³€ìˆ˜ ë§ì„ ë•Œ ìµœì )

#### 1.7 SVM (Support Vector Machine)
- **ì¥ì **: ë³µì¡í•œ ê²½ê³„ í•™ìŠµ ê°€ëŠ¥
- **ë‹¨ì **: í•™ìŠµ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼, ìŠ¤ì¼€ì¼ë§ í•„ìˆ˜
- **ì í•©ì„±**: â­â­â­ (ë‹¤ë¥¸ ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ ë‚®ì„ ìˆ˜ ìˆìŒ)

#### 1.8 Neural Network (MLP)
- **ì¥ì **: ë³µì¡í•œ ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµ
- **ë‹¨ì **: í•´ì„ ì–´ë ¤ì›€, í•™ìŠµ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼, ìŠ¤ì¼€ì¼ë§ í•„ìˆ˜
- **ì í•©ì„±**: â­â­â­ (ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œ)

### 2. ì•™ìƒë¸” ì „ëµ (Ensemble Strategies)

#### 2.1 Voting Classifier
- **ë°©ë²•**: ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ë‹¤ìˆ˜ê²° ë˜ëŠ” í‰ê· ìœ¼ë¡œ ê²°í•©
- **ì¥ì **: êµ¬í˜„ ê°„ë‹¨, ë¹ ë¥¸ í•™ìŠµ
- **ë‹¨ì **: ëª¨ë“  ëª¨ë¸ì— ë™ì¼í•œ ê°€ì¤‘ì¹˜
- **ì í•©ì„±**: â­â­â­â­

#### 2.2 Stacking
- **ë°©ë²•**: Base ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ì„ meta-learnerê°€ í•™ìŠµ
- **ì¥ì **: ëª¨ë¸ ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµ ê°€ëŠ¥, ë†’ì€ ì„±ëŠ¥
- **ë‹¨ì **: í•™ìŠµ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼, ë³µì¡ë„ ë†’ìŒ
- **ì í•©ì„±**: â­â­â­â­â­ (ìµœê³  ì„±ëŠ¥ ê¸°ëŒ€)

#### 2.3 Blending
- **ë°©ë²•**: Base ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ì„ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê²°í•©
- **ì¥ì **: êµ¬í˜„ ê°„ë‹¨, í•´ì„ ê°€ëŠ¥
- **ë‹¨ì **: ê°€ì¤‘ì¹˜ íŠœë‹ í•„ìš”
- **ì í•©ì„±**: â­â­â­â­

## ğŸ¯ ì¶”ì²œ ëª¨ë¸ ì¡°í•©

### ì¡°í•© 1: ì„±ëŠ¥ ì¤‘ì‹¬
- **Base Models**: XGBoost, LightGBM, CatBoost, Random Forest
- **Ensemble**: Stacking
- **ì˜ˆìƒ ì„±ëŠ¥**: â­â­â­â­â­

### ì¡°í•© 2: ê· í˜•ì¡íŒ ì¡°í•©
- **Base Models**: XGBoost, LightGBM, Random Forest, Gradient Boosting
- **Ensemble**: Voting (Soft) + Stacking
- **ì˜ˆìƒ ì„±ëŠ¥**: â­â­â­â­

### ì¡°í•© 3: ë¹ ë¥¸ ì‹¤í—˜
- **Base Models**: Random Forest, XGBoost, LightGBM
- **Ensemble**: Voting (Soft)
- **ì˜ˆìƒ ì„±ëŠ¥**: â­â­â­â­

## ğŸ“ˆ ëª¨ë¸ ì„ íƒ ì „ëµ

1. **1ë‹¨ê³„**: ëª¨ë“  ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ ë° êµì°¨ ê²€ì¦
2. **2ë‹¨ê³„**: ìƒìœ„ 3-5ê°œ ëª¨ë¸ ì„ ì •
3. **3ë‹¨ê³„**: ì„ ì •ëœ ëª¨ë¸ë“¤ë¡œ ì•™ìƒë¸” êµ¬ì„±
4. **4ë‹¨ê³„**: ì•™ìƒë¸” ëª¨ë¸ë“¤ ë¹„êµ ë° ìµœì  ëª¨ë¸ ì„ íƒ

## ğŸ” í‰ê°€ ì§€í‘œ

Churn ì˜ˆì¸¡ì—ì„œëŠ” ë‹¤ìŒ ì§€í‘œë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤:

- **ROC-AUC**: ë¶ˆê· í˜• ë°ì´í„°ì…‹ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì§€í‘œ
- **Precision**: False Positive ë¹„ìš©ì´ ë†’ì„ ë•Œ ì¤‘ìš”
- **Recall**: ì‹¤ì œ ì´íƒˆ ê³ ê°ì„ ë†“ì¹˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¤‘ìš”í•  ë•Œ
- **F1-Score**: Precisionê³¼ Recallì˜ ê· í˜•

## ğŸ’¡ ì‚¬ìš© ë°©ë²•

```python
from src.preprocessing import load_data, preprocess_data
from src.model_selection import compare_all_models, compare_ensemble_models, select_best_model
from sklearn.model_selection import train_test_split

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
df = load_data()
df = preprocess_data(df)

# ë°ì´í„° ë¶„ë¦¬
X = df.drop('Attrition_Binary', axis=1)
y = df['Attrition_Binary']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X_train_final, X_val, y_train_final, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
)

# ê¸°ë³¸ ëª¨ë¸ ë¹„êµ
base_results_df, base_results = compare_all_models(
    X_train_final, y_train_final, X_test, y_test, cv=5
)

# ì•™ìƒë¸” ëª¨ë¸ ë¹„êµ
ensemble_results = compare_ensemble_models(
    X_train_final, y_train_final, X_val, y_val, X_test, y_test
)

# ìµœì  ëª¨ë¸ ì„ íƒ
best_model = select_best_model(
    base_results, ensemble_results, metric='roc_auc'
)
```
